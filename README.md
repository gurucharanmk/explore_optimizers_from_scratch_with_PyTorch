
# explore_optimizers_from_scratch_with_PyTorch

  

### Overview

  

This repository contains implementation of some of well known optimizer using PyTorch from scratch (but not in typical PyTorch's way).

  

### Background

I implemented these optimizer to understand optimizer during my early days of learning, while implementing [PyTorch_CosineAnnealingWithRestartsLR](https://github.com/gurucharanmk/PyTorch_CosineAnnealingWithRestartsLR).

  

### Implementation details

| Optimizer | Implementation Status |
| ------ | ------ |
| SGD | DONE |
| SGD (with momentum) | DONE |
| AdaGrad | DONE |
| RMSProp | DONE |
| Adam | DONE |

  
  

### Results

  
  

## License

This project is licensed under the [MIT License](https://github.com/gurucharanmk/explore_optimizers_from_scratch_with_PyTorch/blob/main/LICENSE)
